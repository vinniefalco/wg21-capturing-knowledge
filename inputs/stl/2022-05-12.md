WEBVTT OK. So yeah, we're doing minimum and maximum and min Max. Yeah, it's the normal loop of just comparisons. For Min Max you get. Ah, you get a different Max element than for Max. Right, if there are multiple. In the array so. I feel like, oh we have algorithm which GitHub doesn't recognize. Let's see. Plus plus I don't know why that is. I thought we fixed that. Can I recommend zooming in so that it will be visible? Alright, and in that case we should probably go back to the unified. View. Umm? Yeah, alright, so this is for. Results and then one function for each size for one by two bytes, 4 bytes and eight bytes. Umm? And I can use the vector out. The vectorized algorithm for like 3 bytes or five bytes. On alright, so here's the actual. Uhm? Templated at three point that picks which one it used to call. There's a bunch of reinterpret casts here. Yeah, so this file is strict aliasing I think. But the. The code it's calling here. Steadman element uh once it is done with. Yeah the code is calling in here. Uhm, it's always gonna be compiled into our uh import library. Or or vector algorithms goes? So it's always compiled with MSVC, so we don't really. Violate strict aliasing, it's OK. Am. And. We don't have a huge amount of choice, honestly. So. And then it's the same thing for my Excel and we just and this one's easier to see because you don't have the big comment. Umm? So if we're a pointer or a null pointer, then we do all the reinterpret casting thing we. We were crazily called ourselves after with. You had pointer which is gonna go to this branch of the L. Umm? Let me just branch on the sides. Same here. Ohm. Here were deciding whether. To turn stuff on. So that's going to be. Work day US. UM? We're not iterating over volatile. I I iterator is volatile. I believe it's an iterator over volatile stuff, not a volatile iterator itself, right? That's right. Yeah, yeah, because then you would use is volatile. Badly needed. Or were integral or pointer and. Uhm? Let's see, yeah, our predicate is less or vangelos. And that. Here we could have used our internal is any of the. Nowhere on 9188. I don't know. I don't think that would probably buy any clarity. UM, it is something that we used elsewhere. Yeah, I don't think it would increase clarity. I think the clarity here comes from the very nice comments that explain each step, so it's not just right. Here's a bunch of in line. Yeah, well, because that could that that would only do these interval or is pointer which is kind of obvious because it all gets on one line. Yeah, if this was over multiple lines. Umm? Or or or had like a conjunct like had other stuff in here then maybe what's on is any of you wouldn't replace the one about his interval that one has to be a disjunction, it it could replace the one that's the disjunction is same is any of the he says is P no, but that would be awful because we we have. We still have the preprocessor thing right in the middle. Yes yeah, for the range of what's the point? Yeah, it'll look pretty ugly. It would avoid repeating his name three times, but. Didn't buy anything. It's NPR, really, but yeah. I'll point it out, but not request a change that seems reasonable, just sort of. Just record the rationale. I think we do use preprocessor conditional with. Is any of the. When we're looking for things like characters types that could be bite because we have them stay catch for bite. So we brought in the same sort of scenario, but it's again. It's the preprocessor damages clarity. Something that really should be standardized, like somebody should just propose this is any of the for this because it's a useful thing with like this specialization fee like is implementers, we keep all the cool stuff for ourselves. I think it's just us in, UM. Write it down. And also. Well, that's an ex utility, right? That's a good point. Yeah, is any of you is defined pretty high up. I can't remember if it's utility or type traits. It's definitely available. An algorithm algorithm has everything, yeah? UM? Yeah, I think it's probably a an instance of like these type traits being so small that like. It's a lot of effort to like do the whole paper and everything. Yeah and everything and then. You know it kind of just comes up on occasion. Nobody's kind of gone through to collect all of them. Yeah, this so somebody would have to go through the various standard libraries and kind of. Uh. Patch them altogether I think. Yeah, like we've all reinvented, always false. That's a very popular one. That one is getting standardized, it is. Or finally, I thought it was wow. It's our time. Well, I had to write one. I had to reimplement that in a in AMP. Which I mean, I guess I could have used the one from the STL, but I don't wanna use SCL internal stuff in app even though AMD thinks it's part of the SDL yeah? Umm? Yeah, let's see. So this was me being an idiot. So don't worry about that. Yeah, this is a if constexpr thing, so it's. Yeah. Uhm? An element. We call min element. Min Max. Comment Max on that and then construct. You know maximum or minimum maximum element. See so before our pointer we return them directly. Ohh, if the if the iterator is a pointer. Uhm? Is this right or do we have to like do fits unfancy and I believe this is correct. It's basically if the iterator that the user came to us. Actually. This is interesting because the context is it's the unchecked algorithm, so we've already potentially unwrapped the iterator. OK, like vector iterator, we have metaprogramming that unwrap that down to a raw pointer. Seems like string iterators then this, then goes and calls minmax element. So if after unwrapping we had a pointer, then whatever minmax. On that returned it returns. Point is, we can just return directly, so that's line 9412. But if even after unwrapping the iterator is not a pointer like it's. A deck iterator or list iterator or something by directional unwrapping's not gonna give you a pointer. So if it happens to be actually sorry in this case, if we were eligible for the optimization, we would need to be at least contiguous. So if you had a contiguous iterator that yet did not unwrap all the way to a pointer, or if it's a user iterator, right? Yeah, a user defined iterator that. Unwrapping doesn't make into a pointer. We need to at this point restore the original iterator type, and that's what we're doing, so it's like 2 levels of unwrapping, which is a little weird. But this is the correct pattern that, well, no, it's not. That's not what this is doing, it just constructing the. Return value which is 2 iterators but yes, but it's 9414. That would restore the iterator type, so. Yeah, yeah, I can use that right? Yeah yeah, sorry no. I think we understand understand each other. Yeah it's yeah it it. It has to actually return the right types so or no, no. Actually it's doing the opposite of that. It's the result is to void pointers. Umm? So if it's not a pointer. We're taking. Oh no, it's returning a pair of four letters. So it's it's really. Yeah, it's user defined iterators that are contiguous, so we can get pointers that would. Be eligible for the vectorized algorithm? Yeah, but they don't. The Unchecking machine for iterator debugging doesn't actually wrap them all the way to if we wanted to. Yeah, yeah. Which is probably indicates we may not have test coverage for that interesting scenario. I think we do, so it's like. This is a wrapper for is. This is presently moving and moving is constant evaluated around somewhere. I'm pretty shocked that this didn't already exist. Yeah, we were very consistently guarding it with CX-20. Yeah. So that's a kind of policy thing. I guess this would be the first PR that uses that that level. Though. Yeah. I'm not entirely sure why we can't just use the built in directly, but. Umm? Are you gonna break courtesy? I mean yeah, like that that's already guarded by could. In fact I'm I was wondering why we have to specifically test for crews? C beer. I thought it was already part of Houston. Vector algorithms. I can't remember why I I think it's that we're usually pretty scared of using built INS just here and there scattered about the source code. This originally came up with the Titrates like double under score is enum is class those they work perfectly fine when they're used in the context of I'm gonna be given to like roll constant within type traits, but if we started just using them scattered here and there throughout the STL, those built INS appearing at different strange places in this. That's the compiler was not always prepared to handle it cuz the builtins are like they're sort of these sort of custom things that the compiler handles. They're not really full fledged like keywords, and you know, really tanked into the compiler, so there have been compiler bugs in the past caused by using built INS and unexpected. That explains quite a lot about our approach to a lot of yeah. So just to avoid bugs, we have traditionally sort of centralized our usage of builtins and intrinsics into helper functions. So we're gonna say, here's the one function. Why there has to get right and then we can call this function elsewhere and then it's an ordinary function call and everything's fine. So we continue doing that even though compilers these days, like MSVC is way more robust than it used to be due to the massive improvements from the front end team. But we're still, you know, for consistency, like to do this and it still helps occasionally point bugs, so it seems reasonable I'm fine doing that. It's a little scary that this whole thing is guarded by Kudo CC, but if we screw it up, you know. You'll notice it immediately, so. And then we get to the meat of the thing. So this is. The same, it's just so we don't have to include algorithm again. One could argue that this could be a simple. Pair we could just destructure it. Up there, but actually no. This has to work in in 14 mode. But yeah, I don't have a problem with this. This isn't gonna change. It's not no DVR violation or any. UM? So. Now we have templated. So this is just the normal algorithm. UM? Let's see. Yeah, and current just comes in through a parameter, and yeah, this is literally just the. Minimum maximum. And then Min Max which is different right we? Umm? Yeah. Use less than or equal here. Uh. Yeah, and it's now statement. Because the mayor of maximal ones can be the same. Uhm? OK. And then there's enough. Which will come up later. Explicitly 1-2 and three it's. It's a, it's a. It's their flags. Umm? Well, that's interesting. Should we have a comment? Yeah, maybe I? I that's actually I that that was my suggestion to make them flag. I didn't put a comment when I suggested it, but yeah, either that we'll come back to that and I'll suggest it again to do it either with a comment, or we could just do put the shifts in. To make it clear, yeah, I either shifts or or comment. I would be fine with either and and if there were more numerators like if it was, you know 1248 sixteen, then the pattern would be obvious, but 123 it looks like an arithmetic progression. Yeah, right? Even if we're never gonna extend it with like, you know, Ultraman and Ultramax in the future, and yeah, anyway. Uhm, yeah. So then. Uh, this is, let's see. And this handles all of them and ties them together. So this is if we're only doing min. Uhm? We'll get to the same thing later. This is we're only doing Max and this is implicitly both the reason that they're flags will be. More. The reason why fly isn't improved clarity will come up later, right? So these are traits, so these are to pull out the size specific. Uh, doodad vector doodads. So. Has portion Max means. Like Umm? It the is the maximum number of. Uh. Of chunks that we can process before we have to. Like actually pull out the real minimum element. We'll get into that later. Umm? And then you know the. Minimum and and and and maximum. Values. Ohm. This flips the sign. Of. Val if you specify size so. Uhm? If sign is false. Then it. Ah. Then it flips the sign, otherwise it does not. So if there are. Yeah, what does core mean here? Corrections ohh OK yeah so this is. This is the top bit. OK, yeah, I understand what the function is doing. Sort of attempting to avoid duplicating all this code for signed and unsigned times 1248 bytes, which is yeah, exactly definitely worth a little bit of centralization here. I was just confused because like core is like. And seen that abbreviation before I I I? I mean I, I think it's corrections. Alright, increment that's obvious, so age funk. We'll get back to this. Uh. Later, so I yeah, so that's the trade we'll come back to it. Should should we just ask for a comment on that? At least the first occurrence of that? Because if it's if it's an abbreviation we've never seen before in the STL, it feels like it deserves a comment. Yeah, for stuff like you know, shuff and. English that those are self explanatory. Uh. Right, uh, yeah so that's that we'll give back to all that stuff. It's pretty cool that we can use the vector instruction like flip the signs of like what was that 16 or whatever elements all at once. That is the. Yeah. UM? OK, so this is the actual algorithm. UM? Yeah, let's see. Uhm? We initialize stuff. Pieces first Min Valley is the smallest possible value. Max value is the biggest possible value. Umm? If we have more than. 16 bytes uhm in our rain. And SSE 4/2 is on. And. The the size of the vertical. The vertical, min and Max portion. Ah. Is this bug aligned? UM? Like, uh, align? Doesn't multiple of 16 OK? Well, what do you mean by vertical minute Max portion? We'll get there. So OK, I guess what I'm kind of looking for is like, yeah, let's do the let's do the overview of the algorithm now, which starts. Uh. Yeah yeah here yeah OK yeah OK so. Do you need to switch the camera over? No, yeah, thanks. Let's actually get into it now. UM? So. If we have, let's do this in reverse order, so let's say, let's see. Save Ohh actually right sorry. So let's say we have. And let's consider 32 bit. Perfect, yeah, so let's consider third the 32 bit elements. On so. What we're gonna do is. So we're splitting it up. Like that, and then we're gonna compute the elementwise minimum of all of those. And in the code this is called the. And vertical minimum presumably. UM? Yeah, the vertical. Presumably because. The idea is that you're kind of forming like. Look, you get. UM? You know one. 5. 9. And then. Ah. To. Well. Laying stuff out like this and then each time you go through you get the minimum of. Of the of the of this vertical column. Right, OK, so you end up. Finding the minimum one that occurred at position 0 Mod 4142, Mod 4, and so forth so. So we keep the state when we're going through. We keep 2 vectors. Keep the valley, the value and the idea X. So yeah, so like. The first time through. Let's see, you would find. Question for those watching, can you see what's on the whiteboard? Just checking so that we've got, UM, the right do we might wanna turn off auto. Camera settings so that when I walk in front of it, it doesn't. It doesn't like right now. Yes, I can read what's going on on the white board. I think you guys need to stop sharing though for the video to work. OK, let's do that because right now it's like I have, you know, just Charlies video maximized and we need that on the video instead of your window with the tiny little video showing up in the corner, OK? And I think, Oh no, it's it's that's turned off. You may just want to write larger and use maybe the green one because I was looking at like the Dal and that's like very very. I can barely see it in person. Yeah, might be running. Yeah, that I couldn't see at all. I didn't know there was something there until you pointed it out. But the red shows up. I think I think that one is cursed and try. Yeah, that was good. OK so yeah so we keep. Now and so it would be, you know. On ohh man, I wrote everything backwards in that matrix because I. Which is gonna make things harder because you get a. The easy thing is it's. Yeah. Like you start, you know you can start with 1211, ten, nine and then we do the comparison. Against. Uhm? Alright and so. Each. Each element, and by the time we're done. We get. Aye. By the time we're done so so. We get. From here we get the element wise minimum. And then by the time we're done, we get the minimum of each column. Right so. Alright can I can I check my understanding that that that's why it's called the vertical. OK, so if our input instead was 12? Negative. 99 uh, ten and nine, then 8. Seven just do 265. Just do two, yeah. Where you like it? I guess negative makes it more confusing. No, that's fine, OK, but just two chunks. OK, like this, yeah? Yeah, then we would compare element wise so then you get 12 to ten 98765. So comparing these two parts and probably standing in front of the camera, if you're getting the minimum. This one wins. This one wins this one, this one, and so we get 8265 for each possible location Mod 4, and at the very end after the whole vector thing has given us a sort of vector of the smallest ones found at each position mod 4. Then we can do just a few instructions and fight. Oh wait, two is the smallest of all evolved? Yes, that's that's exactly what happens. And actually we could implement this for our. For the scalar algorithms, it would probably be faster than what we do now. Interesting because it. It means that the the the loop dependencies are much further apart, alright? Because we can if we're talking between like 12 and 8:00 and 2:00 and 7:00 and 10:00 and 6:00, we can do all those comparisons and we don't need to care about the results of them until much later. We're currently greedily saying OK, we need to. Through 12 and two into comparison and then whatever that comes out, we compare that against 10. That's a huge chain of dependencies. It's sort of the difference between a tree that looks like. This versus a tree that looks like trade, some zooms. Yeah. Yeah, and and and so that that would be potentially useful for like user defined types or word sizes, but. Yeah, it's you trade off. You know the storage space and also at the end you do MLN comparisons. Yeah, which is just kind of. Because vector. Oops, not any algorithm reason anyway. Anyhow, so while you're doing this, you also keep track of the of the indices of the minimum, so. In right because this is min element and so forth and we need to know where or not just would value. Yeah, we need to know where and and that's also true at the end. If we get duplicates, yeah, OK yeah because they have interesting semantics. I I had to learn this because I think we had. We had like a bug report that we did the wrong thing in min Max element. Minmax element is not the same as just doing mid element to do a Max element. It's different to the different minimum. Sorry it gives you the same minimum. It gives you a different maximum and the case of duplicates. Yeah, just because of how the algorithm works, it's inherent to the algorithm. Well, I mean it's specified. That way, yes, it's specified well. Do it with a with a with a greater than comparison, yeah, but yeah, I think you really want to be for comparisons or not follow the STLS prevention of use. Yeah well so the the Max element is a greater than or equals comparison? Or is Max's? Uh, greater than so I think min Max element the Max. Actually. That was more comparisons. Yeah, I don't need to look at it again. I remember to achieve its three in over 2 comparisons. It naturally output a different value in the case of the tie breaker. Ohh God, does that mean this doesn't conform? This one may be fine because it's integers. Ooh, that's a good question about location no, but the N log N comparison thing doesn't matter because it's it's not. It's it's north log N for just the the vector size chunk, so N is constant. OK, yeah yeah that's fine then yeah, we can. Definitely constant factors. I was more thinking about what locations do we return in the case of equivalent answers and even though we're integers, which actually doesn't matter, we do it right. Yeah, OK, good, as long as it does so once we're done once, we're at this step. We need to find Oh yeah, right. So portion Max. Right, so we keep the indices, so for a moment because it's a little easier. Let's imagine we have a really tiny. One bite. Uh, really tiny vectors. A 32 bit vector, right? So 48 bit values. OK, right? So we keep the indices and eventually. We get to the 255th comparison and we need to wrap around, so that's what portion size is where, if you like. That probably wouldn't be your vector, but it could be right if all of your minimum elements were at the very end, like if you get here, you have to stop doing this and you have to rectify. You know you have to do the second part. To pull out the real minimum element here or store it. And then reset this stuff and continue again. OK, I think I think you lost me. So you're you're storing a vector of locations, yes, so there's the index vector. OK, but you're only using a byte for the locations. Is that right? Yeah, this is just so that I don't have to write out to the 32. Oh, I see. So, OK, so you're physically actually storing it. This is exactly happens so. Yes, because we use the same size vector for this and this. Ohh there for the for the locations yeah OK so so so there actually is a mismatch between size T on the platform and the number of bits that we can dedicate to storing indices right? So that's why we have to worry about running out well size T you know 64 bit vector you get two of those. Right? OK, because these are one 2128 bit vector instructions. Yeah, it's to see whatever and and and. Also you can't. You can't like shuffle between. You you can't do the the clever stuff that is done to keep the, uh, the values and the indices and sync and then to tie them together at the end. If the vectors are different sizes, right? Yeah yeah, the same size will need to be the sad, they're the same size and they can overflow. OK, so this is the case of. If we have, is it 4 values per vector or does it? Does it varies. It varies between, right? Yeah, varies by elements. So first if we got a lot of elements per actual one byte elements, you get what? 16 indices and 16 values per chunk. 16 I think that's right. Yeah, OK, so for one byte we get. I'm trying to do the division of my head was that 32 bytes per chunk. 64128 No, no. It's only 1616 bytes per chunk. What 16 bytes per chunk, no matter what, because it's OK, it's 128 bit vectors. OK, I actually yeah, I I would. I would appreciate a table here. So we've got like 1616 byte. Actually they is there an in an AVX 256 version. You know VX two yeah. So it's like we have. 16 Bites, I think there's. And one. And. And the size of teas, maybe the way the camera has fallen since I put it. Ohh no. You sat on the chair with the camera. This is like yes Oh yeah no it doesn't. Yes you can. It doesn't have this one really great on YouTube. This should be our thumbnail. Video editing please. If yeah, OK, yeah, just don't don't. Bigger trouble just in case anybody out there thinks that we're not software engineers and that we're actually just, you know, content creators. We've proven it. OK so I was wondering 16 bytes on size of T could be one to four eight and so then. Elems her portion. Of the terminology or junk, yeah, I. I think chunk no yeah chunk because portion is or portions different. Yeah OK portion is the number of chunks. Ah which makes sense right? You when you're eating you one chunk of of food at a time and the number of chunks is portion size. OK, so you can have 16, eight, four or two UM. And if you are because this works on both X86 and X64, so for X86 we would need 4 bytes for pointer and then for X64 we would need 8 bytes. We're not storing pointers right pointer size? Keep saying same deal so here. If we have 16 bytes in a chunk and we have two elements, then that is exactly OK for storing size T, so we can just store no true size directly, no, no, you can't because each element here is going to be 32 bytes, each element is going to be 32 fights, but I said what the size of T is 80. Sorry, sorry yeah, you're right, you're right. OK, so in this case. The size of T always works, and in this case we'll reach 4 elements. Then we're camped at. 16 / 4 square. So we're captive 2 to the 32 times that. That's fine on X86, but we do run into overflow on X64 here. We're capped at 2 to the 16, and here we're capped at 2 to the 8th, so that's the 2:55 you were talking about. Yeah, OK, so I see. So if as the. Element size gets smaller. We need to pack more indices into each 16 byte chunk that the vector instructions work on. Well, no, that's not what we do. Make the portion smaller. So once ohh I see once we get the whole point. Yeah, one point we use exit the vectorized loop, do the next part of the algorithm and then store the actual minimum value. And the actual minimum value and index. In just stored normally, just like with, just like with the old style minmax minimum maximum but OK. So this is essentially and then reset when we enter again or redo the tail which is just the normal loop. OK, I'm trying to make sure the camera can see it, so in the case that we've got like vector of T and we've got. She happens to be like 1 byte and we got like a whole bunch say this thing is for concrete purposes 10 gigabytes, which is a totally reasonable size. Since we can only consume up to 256 elements per portion, you're saying that we're gonna run the vectorized loop only on like the 1st 256 elements. So use cool vector instructions, find the minimum out of this region, exit the vectorized loop and then we run the vectorized loop again on the next 256 and then we find OK. Is there a better candidate here and so at each point we're finding a current best known candidate. And for minmax, it could be a different locations and we're still getting the advantage of the vectorized loop because we get to consume 256 bytes of time. Yeah or more in the case of larger elements, but we don't need to do anything super fancy to like cleverly split the index or something. We're just saying OK, because we can only store up to like 2 to the eight or two to 16 or whatever. That's how many we can consume for vector. For that does mean the end is actually part of the asymptotic complexity though. That the N so so let me explain what happened. With the it's just like that's a bad one with the once we're done so. So once we're at this part, we need to find the the minimum of these elements. Right? And we could do that with a simple loop, but we actually do it with a bunch of simple instructions. So we we go. Umm? Like so. Umm? So we go like this, right? So OK, like conditional, not conditional. So first we shuffle this vector. Here, right? Actually the order doesn't matter and. For some of the sizes, it's. And for some of the levels it's increasing some decreasing, but it doesn't OK. I, I think this is the order for, but anyway so we get 3/4. 1/2 OK and then we do and then we do the minimum and so we get. Minimum. 43 minimum 3. UH-4 21. Oh, I see by shuffling it and then comparing the minimum, you can compare the elements against themselves. Yeah, OK, and then we're just they're they're counterparts essentially, yeah. And then we do it again. We shuffle this. Again. Wait? Yeah, OK, if you shuffle that again then you can compare whatever the minimum of four and three was against two and one was, and then the output will be. Whoops, yeah, you may just come. The output will be all identical and they'll be the minimum of exactly values, and they'll be one on one one in this case, and then you just extract whatever element it's because, Yep, OK, that's exactly right. That's interesting, so that's what this is. So this is let's go to the one for four because it's kind of easiest. That's a win if we're doing like what up to 16 bytes at a time. We only need like 4 swap or 4 shuffles or something and minimums rather than it's always a win. Ohh cause no branches, it's branchless, no branches, no data dependencies. OK that's fancy OK so there may be some situations where it's not a win and I think the other thing. So actually that that's the. Yeah, so this is for 32 bit, so we do. Yeah, so ohh yeah. So we swapped the first two and then the 2nd 2. Each min shuffle hmin yeah. When we shuffle against itself. OK, this is very enlightening. Otherwise I would have looked at their shuffles like what? What are we doing here? So that's what's. And then, so here we're doing. Uhm? 2301 Yeah, so it's just swapping, OK? Like it's just swapping sort of a halves of yeah OK, because they're in order. Yeah, this is and that also explains why the shuffles get smaller as you go down and and also to to point out this isn't an order right? Because so here we're starting with swapping the 32 bit values, but if you look up. At. What we do? Should we go back to sharing? Yeah, we should go back to sharing the screen. Can I reclaim your chair? Yeah, so but it just switched the camera over here. No, give me one second. So yeah, so if we look at for the first one, then we do the same thing As for 32 bit. And then we do the words and then we do the bites. So we're kind of doing things in a different order, but the order doesn't actually matter, because because the shuffles are order independence, they're essentially, is that right? Alright, no well, as long as you hit each element right min. Kind of. Doing man of men of something right? You just add the something to the big. You know the big man? OK yeah, let's switch. OK, I I now feel prepared to follow along with the rest of your code. If you the camera. Yeah, let's see, so there's some. Yeah, so and then for this uh, let's see. So I looked at it and. So the sets. So these I think have. Intels intrinsics. Intrinsic. Talks aren't that good, but. Which one is it using? Umm? This is said that the eight right? Yeah, right. So there's. Yeah, yeah. Yeah, Intel has like, uh, three point latency statistics on that page, but they're like. Little dubious. Umm? And obviously like you can have an instruction that, uh. It's like has more latency than you'd expect because the the the resources are just utilized. Yeah, so I think in some of these there's like one or two cycles at the end. That we could stick something in and that are. And where the next instruction? Is dependent. On on this funct. Ohh yeah funk is so that it can do minimum and maximum. And. You know how is this? Was that templated on ideas. OK, so we're using a. A function object as a template parameter. OK, yeah, it returns right. So H funct. Is this? And then the actual things. It's not a functional object. It's a Lambda. OK, I guess is a function. A Lambda is a function object that the compiler makes. Yeah, OK, that's that's that's nice. I mean it's. Reducing the amount of code we have to read by three, so that's very much appreciated. Yes, if only we could do the rest with all the 816. There's also some differences for eight, OK for 64 bit. Uh, the appropriate instruction is a the X512. Thanks Intel. Where is it? Yeah. Yeah, so men, so here this is 32 bit. We're using M min API 32 which is. So you see Mm-hmm so DPM Inc. Is AVX 512 and we're doing so? Yeah, so MNE PI32 as wow. That's been a lot of like anyway. On instruction latency, but then for 64 bit we do. Some blends. Yeah we do and then blend the. What is a blend? Is that combine 2 vector registers with the one? Yeah. Uh, using a mask, which one is it? It's API, yeah. UM? Yeah, so it it it has a mask of. M128 mask. And the top bit. Of the most significant bit of each. Byte in that mask vector determines which which source vector. The value will will come, will be taken from OK, so it doesn't combine the integers themselves, but you do get an arbitrarily pick, which might you're taking from each vector register based on the mask. OK, right? Yeah, and the reason the mask is a whole vector, like for for the comparison instruction. The result is a vector where. No, the the passing comparisons are set to all one. Passing comparisons, yeah, so like if it's a greater, less than. Yeah, it's less than the ones. Yeah, OK, I see the the result is like 11111 and then just means OK this whole byte was less than this whole other byte, OK? So because I I'm imagining uses these vector instructions often, find it useful to have ones in all positions. So rather than requiring more vector instructions to spit out additional ones, or like sort of widen those ones. Yeah, and there are, there are instructions to especially narrow the ones. OK, so you go from a massive vector to a mask integer. Or a mask scalar that's a bit field. Uhm, yeah, that's very useful to that. Alright? So this is dealing with the wrapper. Ohh, the reason we have have portion Max is for for for 8. Uhm? OK, so has portion Max is essentially the the case where we can't just consume the whole input all in one class because we could run these. Yeah and we can always consume the whole input, right? That's because the elements are so large that we have so few elements that we can always fit in size 2. So for for my own enlightenment, can you look at the traits for four? Did we get an X? Do we get an architecture bitness dependency on portion mask masks? That's two, yeah we do indeed. OK great. So this this corresponds to what I would expect. OK, OK, and then the portion is 1 bigger than or exactly 2 to the 32 OK? That that that aligns with my understand. Ohh yeah, that's a bit. Yeah, they could have been written as one left shift or one UL left shift 30 yeah, but I'm part of that too. Yeah someone's nice, nicer net that we have digit separators we ever live without. Yeah so. Yeah. Let's see. Max portion size is. Right, because we do four in one go. And I think the the indices are indices into the vertical. OK, so. It's. It's indices into these into these vectors. OK, realize I'm pointing at something that nobody can see, but pointing out that the matrix of. That sort of stacked input vectors. OK, yeah, why is it? Why is it multiplied by 16? Or is that 16 coming? Let's see this, oh, this is the general minmax element, so we don't have any. We don't have any size dependency yet, but it's saying that since we're working with 16 byte. Chunks that portion Max is the number of portions we can consume. So times 16 is the number of bytes. Is that right, yeah. OK, OK, and a portion size is indeed a byte line. Yes, OK. Ohh yeah so. Yeah. OK, I really like the name byte length for the function. I don't know if I like the name portion size so much because the the. The question is like interpretation. There's a very strong assumption in the STL that when we see size, we're counting elements. And if anybody's talking about bytes, they really need to say bytes up front. So that's why I love seeing Bike lane. But portion size is like portion bite, size, more self explanatory. Better to err on the side of being a little bit too verbose and constantly saying right, because the danger of not. Saying so, I don't know where else needs to be renamed. Well. Umm? And because Max portion size. Or put Gates portion maxes elements exactly exactly. Yeah, portion Max is elements and then Max portion size is talking about fights then that is. That is very very easy. It is essentially what we're trying to make up for with naming. Here it is the lack of unit system. We're converting between units of elements and units of bytes, and we don't want our spacecraft to crash because we screw up the units. Yes. OK, this is making more sense now that I'm understanding the terminology and so then yeah, and then we just truncate it off so that we do the right number of things. OK, then we advance. Ohm knowing if I stuff. We load the first chunk. Load U, that's, uh unaligned load. Uhm? Yeah. And we do the sign correction. Uhm? We initialize the minimum values to the current. The minimum maximum values to that first chunk. Because they're, by definition, the best known so far. Yes, and then we initialize the current indexes current indices to 0 because that's the current indices. OK, it's in. It's a chunk index basically, so they're all at the same yes, or alternatively, a vertical index vertical OK. Yeah, so I see cause I see vertical comes from thinking of it as a matrix when I think so yeah, instead of writing the whole vector out on a single line like I do when I'm writing, you know linear vector. You're just sort of wrap around and now you write one line per. 16 by chunk basically, and then, the vertical index is OK. What row are we on? OK, that that makes sense that I was confused why at first you're writing it as a matrix and but now it makes a lot more sense, yes? Oh yeah, and then we enter an infinite loop. It's not actually infinite loop. Let's see. So let's see what do we do? Stop at yeah, so that's the end. And then we advance the first by one vector. And we increment our indices. So increase all of them at once. So this first condition is if we're done, so let's skip that. UM? Let's see. Timeline times like this that I really appreciate. Like VS codes code code folding the yeah they all they added colors nice yeah so yeah the bracket colorization is awesome. Yeah well so then we load our next. Our next set of of of things and up at the top. We initialized. Yes, yeah, we initialize the minimum values to that. Might be able to squeeze some exercycles out there, not repeat the work, but. Umm? I don't really have a problem with. Yeah alright. Yeah, so then we. If we're in a minimum mode. UM? This is not equals to zero is apparently a compiler bug. When it raises a warning, if you do that flag style thing, and if context per interesting, what warning does it emit? Uh, see 40 it's. Like comparison is not a bull or something. It's some W4 warning that we don't suppress interest. Because elsewhere we've directly tested. I mean, it's not. I'm not actually. This one is superfluous. That's horrific. That's it. Else find the estl we've been able to directly test the results of wise and on them. Directly, I've never been super comfortable with it, but for you know it seems reasonable. All right so. If this is a foreign Max only mode. Uh, than we do, yeah OK so. Is last we do a greater than? UM? Between minimum and and current value. Uhm, and for Max we flip it. UM, and then we do, UM. Ohh this is yeah OK. Why, why we're doing naughties less? Why is UM greater than being used here? It's just stylistic. Yeah, that's I think, let's see. Uh, there's E here and there's GT, But there's not Lt yeah. Let's see if there is. Yeah, you one could use Lt. UM? Yeah, I find that surprising only because the STL has a historical convention of really preferring less than. So so to see a greater than being used. I don't mind that here because it's the like vector code and. The sort of. Delegate to less that you only have to implement less doesn't really apply. Ohh yeah, I mean I would be fine with seeing breathlessly and greater than being used. I find it surprising that. One would be used and would happen to be greater than right? I think I see why because you don't need to. Yeah, both the traits but the yeah and also you don't like. You're also not doing like you're not saying not greater than, you're just flipping the the order of parameters around. Later on, so yeah, so you can get less from greater, yeah? But I still find it very confusing that greater would be selected rather than less. It's not necessarily ask for changes. Yeah you, you're just selecting a different one. I yeah I. I don't mind it at all. So once you do the comparison. So is less is current values. Is less than. Valves man. Umm? And then you get a vector like like I mentioned of all ones. And then to get the index. You know a blend V. On the current minimum, indexes. The current index which we just bumped out all at once because if it is then you want to select that index. That's only gonna be constant ones. UM? And and the and and the less than value. And then. You do the same thing for the values. Umm? See. Ah. Yeah, OK, so this is actually a little bit interesting, so notice you're getting the man and you're passing in his less. But if we look up at the trades. UM? Uh. For 32 bit. We don't use it. We don't use that mask, we just call UMMNE PI32. And that's where. But just directly takes the mural of each, yeah. And that's because MNE PI64 is in a V X512, and you'll notice. So this is so. This is 1 instruction, and then or I throughput and latency of 1. And the thing we do for. Uh, here is blend V. Uhm? Where we just select based on the mask hmm and that is. Is it API? Yeah it is API. And that's two in a lower throughput. Skylake, so it's a little bit slower. Now. UM? Yeah, and it. It also has a better dependent, uh, better dependency chain. So that's why we have to pass that in. Intel said I really wish Intel would ship the APX 512 stuff. On processors, adult support the actual 512 bit vectors. UM? Yeah, so we select them and we update that and then we go to the next iteration of the loop etcetera etcetera. So we do that. And now it's time to do the horizontal part. Be it to stop at. Where we finish. The loop that we accident and do the tail. Uh, But that's taken into account. Let's stop that. So we we we get to to stop at and this is the. Consolidate the shuffling thing. So if we're in. Minimum mode we do admin. And then we get. One of the values is there all the same. UM? Or actually get any. Did I complain about this? Ohh yeah CVTS I that's fine. OK, yeah I was complaining about get the pause. But I was wrong. Now let's see. UM? Yeah, so this does the convert thing which basically just truncates. To wine. Whatever. OK, it takes it takes a vector of values. We assume that they're all identical and we're just like we just want to scale out of it. I don't care which one. Yeah, OK. I think it's always the first one, OK? Umm? Uh, let's see. And this is doing the classic. Excuse me, this is doing the classic comparison loop comparison. Update them in and then. This is tying together. The indices so. Ohh let's see so this is. Yeah, so this is the previous best horizontal man and this is the so we get the equal values. Move mask. Takes the vector mask where where elements are set to all ones and if the high bid is set an element. It'll set the bit in the resulting integer to one. So you get a 32 bit integer from that, OK? Umm? And I mean really unsigned it, but like. That's that's E. You said that this just sets everything to to one. And then we take. This is the case if if if yeah, if there are. If there are equal. If any of the values are equal. Then we can pull out. Let's see. Yeah, then we. Pull out. The current index. Otherwise. Take the maximum index. Ohh ohh yeah so then? That's why we pulled out the ones. So then if. We had equal values. We need to make sure that if they are in different columns, we still get the correct one. So we then take the minimum of the indices. OK no cause we wanna find the first one for a minute element. Yeah, yeah. And then. Uhm? Let's see. And then we pull out. Let's see, you can't be equals. Uh huh. Yeah, this is definitely definitely doing a lot. I don't immediately see what it's doing. I might get the general idea, I I I, I figured out what it was doing on Monday and I've now forgotten, but that's how I was with, uh, with Kerry yeah, it's tying together the indices and the. The values and then it does a bit scan forward. And then it takes the result of the BIT scan forward and and extracts that element from the vector values. Yeah, for for me this indicates the sort of test coverage that I'm gonna wanna see and I see that there are tests being added here that need a lot of attention to not only different element sizes, but also the duplicate element case that we're looking at both the values and the indices coming out. Yeah, especially when there are duplicates across different tests are are. He generates a bunch of tests and then compares against. The old algorithm OK, which is which is. Yeah, so then we take the base and then. Vertical Pass, which is the stride. Plus the horizontal position OK. Because we need to know which chunk were we in and then OK, we're looking at like the third element within that chunk, OK? And then if we're doing Max, we do essentially the same thing. Let's see. And then this is just. Uhm? To do the different comparisons. Yeah, initially this was a ternary inside the if conditional, which I complained about. Yeah, that would be confusing. Let's see. Yeah, OK. And then here we've got the difference between Maxwell element and Min Max element. So this is something I wanted to see, so I'm glad that it's here here. Max Element finds the first maximum min Max element finds the last Max. Yep. That's why it's H Max. Instead of men. And then the same. Thing there. Let's see. So if we have the portion Max. Uh. This kind of repeats some of the stuff done at top. I think Alex was asking if I, I think the best way to if we wanna eliminate this repetition would be quite use. Go to. Oh yeah, I'll label break which I actually don't mind. But yeah, I'm I'm. I'm not a big caterer of go to. Yeah I I am so I'm OK with some repetition here alright and if necessary like we could think about it. I you know I yeah. I mean it would be a go to that's like a go to use this like a labeled statement which I think is. No, not not. Not the previous opinion, but in any case. UM? Yeah, so we're gonna see if we are at the end. And it equals 0 means that this mask. I mean, either we're at the very end, or this masking masked out all of the bits in the size. In which case where the last inside that last vector element. And then we just do that computation again. And advanced stop at. Uh. Set base and we've already advanced first, I think. Umm? Load current values. And then if we're in minimum mode. The update. Men and IX men, and if we're in maximum mode, the update Max and I like that branch could be done at the top too, but. Uh, there's no reason to do it. And there's no way. You know? Like these branches are OK because they're all, so they're gonna get predicted they're in the loop in their constant over the holder. So yeah, and these these material, we've constexpr arts so, oh that's true too either way. There's actually some argument to be had for changing some of this stuff from if constexpr to to if. Because in I think there are certain cases, especially in admin. So for example. If each min could be dynamic on the size. Right? And. The UM? If the branch is speculated. Then like you. UM? You have a couple of cycles. The the instruction right after it, right after each func is called, has a dependency on this final min instruction. Umm? And. I think min has. No, no, it's only one second latency. Umm? Yeah I did. That there's some cases we're doing. The If is better because. There's no reason not to, basically. Yeah, but also the the numbers on the Intel and intrinsics guide are not sufficient to determine that. Because they're incredibly dubious anyhow. Uh, let's see. Yeah yeah, we advance if we're at the if we're actually at the end. Now we're at the end of the selves. Uh. Ohh yeah, so if we don't have like what? Ohh yeah, so this is this is a 64 bit case. You just break immediately. And once we update things, we continue. Which I kind of thought was confusing in terms of the ordering of the loop. It seemed like it was this. The thing that essentially reruns the vector algorithm for each syllable portion. Yeah, this does the updating part of that, but then it continues up to the top. Starts it again. Eat another advances. Yeah, advances first, after it does all that stuff OK, but I think that's because it has to initialize the values first. Right, so because the first? Well it's because the first iteration of this trunking algorithm is is unique, right? Because you thought you need this load everything and say hey we found the best stuff and if you do the comparisons that it's wrong because you just compare it against zero or whatever. Yeah? So yeah. Ohm. Yeah, and then we're done. And then at the end. We just do the tail, which is the normal. And then we do this. I checked that all the numbers match up. It's good. Yeah, because at the end you don't want that copy paste or you saying or when you meant eight. Yeah, so for the test we had last. Last good known me in element. So this is the normal. Naive, Lou. UM? And this is a the normal min Max loop. We verify that that's the same as what we do in minmax. Uhm? Yeah, and so he tests. He generates various sizes of. UM? Of vectors. OK, so they're saying only one per, so I I think we need to make sure. Yeah, we need to make sure that we get the actual special cases. Yeah, it's it's random, which is good. But the the extreme complexity of this code, I think, demands. A little more focused testing. In addition to what's here. The way that I was less concerned about with like finding count. Yeah, because especially because finding count didn't really introduce. He sort of, you know, shrunken portion and entities whenever, whenever we've got these magic numbers like you know, 16 in the code that creates that. We've been bit by this before the. And what was it? We ran into a bug where it was a sort. I want to say it was a ranges. Either range of sort or stable sword or partial sort. I think it might have been stable sort. We only tested it for like 1617 something elements. There's a hard coded constant in there that when we get over 32 we do special fancy stuff and the code didn't work for large numbers of elements. So whenever we see a constant the code we need to test on the side. Very, very well called coverage. How does playing code coverage work? If you compiled the code, the actual CPP file with MSVC. I have no idea. I've never run client code coverage. In theory that would be an interesting metric to capture in practice because we're so heavily templated, we've never really explored code coverage. Not saying that it wouldn't be bad, or would it be good, but we are so far from having that as a heuristic for a metric that we could look at. I I would I can sort of. I mean, yeah we could. We could bench coverage, but I think I think that the asking a lot. Yeah, I think so too. Here I think. Yeah, these are, yeah completely equal vectors vectors of equal chunks. I'm thinking focusing testing on the duplicate cases would be very interesting cases where we know we're gonna have duplicates in the same positions or the same. Modulo and you consider this. The code is reasonably like it's been fact about, that all the trade stuff is centralized, so I am not as concerned that we're gonna find something that only fails for like 2 bytes and not for 8 bytes. There's a little bit of that because the portion size, but having at least some coverage that OK if we've got a duplicate element here and here and they're both the minimum we need to find the first one. We're running minmax element, we need to find the last one for the maximum. Yeah, just having one case that would really make you feel better about. Yeah yeah, this is this. Definitely good. It's a good start. Yeah. And so the other reason why I think that we need a little bit more interesting is that we're using. The random number generator, just as a quick way to spin out. Sort of, you know reasonable looking coverage, but it's specifically not a randomized test case that's generating new numbers every time we run it. This is deterministic, preceding it always with 1729 or whatever he chose the seed to be. So every time we run this program, it's always going to generate. Only generates 124 OK. Yeah, so we do get multiple portions for 8 bits. OK 8 byte, 8 wait. Actually do we because they're in the trunk so you might not. Yeah, because they're their indices into the column, so you would need 255. Times 16. OK, that's pretty. Yeah like four 4000. Yeah, I actually don't think. Let's see, and then for two. 65,000 that's alone. Now we do need to worry about. OK, how long does this take to run? So like for the for the cases like either 2 by where we've got, you know 65K or something, maybe a little bit too much to like. Test automated having like a little bit of manual testing would be reasonable, and of course 232 is trying to extreme. But certainly for like the the one by case, I think we could test the yeah rerun the portion there. Because that's definitely it. Yeah, it's it's. It's the worst in the sense that it's control flow. Which state and control flow are things to worry about? Yeah. I mean, if necessary, we could explore extending this test to be truly randomized to the strategy that we've used elsewhere, which is like the sort of the the second highest level of stringency is. We get the test to generate random values at runtime, and we have to log the seed, and then we say OK, if this test fails, do not ignore and rerun it. You must report this to the maintainers and give us this seed, and then we can replicate that failing run up, and we do that for CARICOM. We do that for a couple other tests where there may be such obscure failures that the cumulative coverage of running it over and over in the harness could grant stuff here. I don't necessarily know that we need that, but that is something that we can have. The ultimate is pure exhaustive, just like every possible value for min and Max element, it's not. It's a lot harder to do because you got to worry about duplicates and non duplicates, so you can't just like test all combinations is there in one. I don't think exhausted is the reason we're here, but certainly having some focus test cases of this is the same. All the portions or whatever. Complexity, yeah, we're we're definitely gonna fudge the complexity requirement. The fun thing is that if we know that it's less, then they can't observe how many times we're revoking their predicate. And are they going to complain if we come back to them unless wall clock time that they expected because we ran a vectorized algorithm? As long as we're not like messing with the asymptotic complexity, which would be very bad, we are though, are we changing end to end login in terms of and the number of elements? Yes, the case where when when does that happen? Because if you have multiple portions you do the horizontal part multiple times. But the horizontal part is constant time for number of chunk, right? Or for the size of the ports? Like the size of the portion is determined by the element size, right? Yeah, but it does end log N comparisons. OK, so I'm working well it does, you know four log 4 comparison? Yeah, but that's constant in terms of the number of elements, right? Sorry elements size. Yeah, but you it's times. Ohh. So what I'm saying is imagine you've got a one byte element. We know that the portion size is 256 elements so we will run the algorithm N number of elements in the whole thing divided by 256 times that many times. We're going to run the vector algorithm that's on over 256. Then you're saying we're going to do this extra. That is, that is going to be fixed in terms of the number 256, so it's constant, but. It adds. Like it? It still adds like an act like a. A number of comparisons that depends on. Yeah, yeah. It's essentially saying instead of running like exactly in comparisons, we're around like you know 1.2 or 1.01 or something. And yeah, but it's still linear. It's a constant factor. Yeah, yeah, yeah, and since it's not really observable, I think we can get away with it like I view this. Yeah, it's definitely I. If you complain about this, like. Yeah yeah you want. Do you want us to make your code faster enough like I think what we're doing here is strictly less weird and standard bending than our debug comparisons where we do take things like hey, you know you're only allowed in comparisons of the issues to find predicate, we can only find do it twice only. Find the complexity when we make the code slower. Yeah, yeah, yeah, very few people complain when we get past. Let's see what is the types, I don't know if camera is camera. I'm trying to find the on the compiler warning. Yeah, one mode was a clean. You know, I'm not an enum class. Yes, oh, I think it was in there. Yeah, it's a normal, you know. So in I guess. Yeah, the underlying type. The default to be interior and then mode itself is another, so it's you know equals enum or enum and enum and enum and the actual variable was a template parameter right? I mean non type template parameter. Yes, I suspect that's significant as well as a constant. Was the warning something like conditional expression was constant or something? It it's an if constexpr prayer. Yeah, the computers had a little trouble with or the best acting classes had a little trouble. Maybe fix it that that was just in with certainly wouldn't be the first time if Constexpr complained about a constant expression. So essentially it would be for a while. Tyler warning and with the self contained repro, we should just report it's part of you know that the east wall is one is egregious and probably not necessary. Yeah, that would definitely need to be fixed. At the end I would like to at least understand what we're trying to permanent work around there before permanent work around. And playing Jack, it is yeah 5:30 so we should worry about dinosaurs. Yeah, this is very enlightening. I did not understand this PR when I first glanced. Any like an entire day at least? Maybe like 2 days to. Well, an entire day of work like spread over a couple days to actually. Decipher what the hell is going on. But I mean the actual algorithm is not that horrible. It's kind of. It's just having to handle all of 145 sign in and sign. It's kind of well, I'm going to call min. I'm going to call vectorized min in chunks and then what do I do next? And it kind of leads you to this conclusion, yeah, but the speedups from the vectorized algorithms are so amazing that it is well worth especially for this this, this really cool. Yeah, this is because you're breaking the the inner loop. And anything if you do the naive wouldn't even know that. I don't know if we were doing that before, but. If you do the naive loop, each iteration of the loop is dependent on the last iteration. Yeah, like nothing better. Do you like one comparison? You like one you like 2 comparisons, right? One for the actual comparison, and then one for the loop, and then you just sit there on the next comparison. Wait, yeah, waiting for the result to be available. And I actually think implementing this exact algorithm in scale our code. Might improve performance, but now that we haven't or might improve performance even when we can't use the vectorized version, yeah unfortunately. Then we start running into the if somebody's noticing how times are revoking their predicate. Then we do start losing. Yeah, well when actually no because we don't have to do any login. Because the same number, no. So what we're duplicating comparisons here. I see if you just rearrange the order in which we do the comparison. More of them when we do it. If when we do the comparison, the size of the type increases. Right, so if we. Like if we do. This. I let's see. Yeah, so we do this comparison and we get this and then when we do the next one. We we only compare. Like we just only do this. Right and the minimum of three and four and the minimum of four and three are the same. Hmm, we know that, and so we just. We mask those bits out and put it that way. Yeah, we would be avoiding the very long dependency chain by doing them in a different order and sort of keeping more results suspended in the error. Yeah, more complexity on that. We need to worry about wait, what if we end early? We don't have multiple 4 or whatever elements but could be feasible. Yeah, if that would keep the same, so that's an idea. Even for user defined types of course. They also you know if it's a user defined type and it has a less operator and the less operator doing a bunch of stuff then it might be that you know it's kind of enough. Yeah, yeah, at that point we have questions like is it diminishing returns. Is this really worth spending a lot of information complexity? Whereas Ohh, you're running min element unlike integers, you know integers are super common. So yeah, I mean this is like an order of magnitude speedup. Yeah, we will do almost anything for an order of magnitude. The common types. So yeah, that's an idea. And then there's, you know. Twiddling around with like? D vector or D templating. This a little bit and rolling some of the stuff that's done at compile time here into runtime. That'll make stuff slower. Either it either won't matter, it will make stuff slower and absolute characters on a micro benchmark. If you're only testing one element size. But if you do, if you call these with multiple sizes. In a row it it could help performance. Yeah that might not be a pure win. So yeah, I believe that it's a lot of work. And it also might be very CPU specific. I'm sorry, yeah. This changes, I already requested changes so. Yeah, that's kind of the. Because a lot of that is like tucking. Like tucking branches and comparisons and stuff in. After specific vector and basically saying oh, the processor is gonna be busy doing this other thing, so I may as well keep all the execution units. Yeah, I may as well do this. I may as well do this extra work. Yeah it's like in exchange for not for you know not not not duplicating the code with templates. I might just do this extra work when I might as well and then. If I use it with, if I use different sizes, everything's already there. But it's yeah. I mean, this is not a lot of code. So yeah, I I don't know how big these instructions are, but. I think it's a lot. It's a lot for maintainers to understand, so I'm happy with this as an initial improvement. Get that order of magnitude and then later on we can think about potentially tuning it more. Worried about like the correctness issues? Ohh great looks like. Already been fixed. When was this fixed? April 8th. Wow yeah, so yeah, this would not yet have shipped in a public preview, so Alex would not have seen it. Didn't we just, ohh yeah, it's. But we're the VS release is so we're going to be shipping on a fast frequency, but the latency is significant. So stuff that was like checked in and should we just give it a transition? VSO, yes, that would be reasonable to site, but I would cite the where the PR number. Yeah the PR number. Because that way we can look at it so MSVC PR blog because that way we can go to directly say, OK, Jonathan fixed this in April. It's available. Actually, we should know when this is gonna ship. This is gonna be 17 to preview two sorry yes 17. Two preview two. Yeah but no seven sorry 17 three preview 22 just 17 too many seventeens 17. Two just went general viability 17 three Preview 2 is the next one that's going to contain all sorts of awesome fixes. But we have to specify date in the future. Yeah, we have to be one behind though because. Premium mobile like we can't just make the change now no. No we can't but but the the transition comment could be ohh transition comma 17 three Preview 2 why do you do this and that will be the reminder that when somebody goes and updates the tool set if it's 17 three Preview 2 to go get rid of the work around let's see where is the thing. I think you could look for mode and I think you might have just. Pass one of them. Yeah here. OK. Auto linker doesn't do the installation. Yeah that yeah. Also, it's a common. Yes, I do love searching for that. Uh. What can I put to separate, recommend, cut out the MSVC bit, make the comment just say 17 three Preview 2. That's all we need in the source code. Then if you yeah, close the code block and then your comment here. Psych, MSD CPR law and that will be. So that's good. Yeah, this is part of our long term strategy of preventing us from drowning in an endless sea of compiler workarounds. Like we always get some, but we gotta get rid of them at some point and consistently commenting them is the way to avoid having tutors that end up being like a decade old. Or just weird bits of code that nobody can remember. What we were doing. Yeah Ohh Karen asked what the rationale is and I think we've covered it. Which is that it makes. The Min, Max and Min Max algorithms like 10 or 20 times faster, yeah? It's a huge speed up. And yeah, and the rationale behind why that is, it's like. Vector instruction is good and. Huge dependency, you know interloop dependencies. Not good, yeah, and the ability to make code automatically fast when somebody upgrades is like just the the best thing ever. And not only is it an interloop dependency, but it's also kind of unpredictable, yeah? Yeah, if you have random data that that comparison is gonna be. It's gonna be taken in half the time, yeah yeah yeah excellent. Excellent point really so so you you you really can't win there. Yeah, so this is a substantial speedup, and the proposed future directions are in a VX2 version and an AVX 512 version. We have to benchmark it, because obviously moving up to bigger vectors uses more memory. Yeah, and also. And so with the previous 8X512 PR that we were unable to find actual speedups from that, so I if P. X2 has legit actually speed up, yeah, 3X2. We've seen quite good results 3X512. We would definitely need to make sure that it's going to be faster before committing any code there. Yeah, plus, it's available is still quite limited, but for AVX, 2 AVX 512 it would be nice if there's a couple AVX 512 instructions that would be great to use, but. Yeah. So yeah, that's not PR. It's a neat PR. Hopefully we can get it merged. Relatively soon, yeah, I'll take a look at it soon. Hopefully we can get it soon for 17. Three preview three. Yeah, which is the next thing that our code is flowing into. Yeah, and then we'll eventually have to port it to neon. Yes, there's a separate PR for extending all this to R64. Well, it only does reverse. Which is what we had vectorized before Alex cause teeth ended stuff yeah but yeah yeah we're gonna need to you know have additional follow up PR's too. Yeah but. Yeah, I mean now that we have the structure of this code, doing it in neon is good. We do need to discuss. Our general strategy for once, we have vectorized algorithms for multiple Isas. But yeah. That's for another time. So yeah, I think that about wraps it up. Cool thanks everyone. Yeah, any remaining questions. It looks like that's it in the at least put everybody to sleep. And yeah, probably stop the recording now, yeah?